{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from scipy.optimize import fmin_bfgs;\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, X, y, lam):\n",
    "        self.X = X;\n",
    "#         print(\"in constructor, length of received arguement: \", y.shape)\n",
    "        self.y = y;\n",
    "        self.lam = lam;\n",
    "        self.m = X.shape[0];\n",
    "        self.n = X.shape[1];\n",
    "        self.y = np.reshape(self.y, (self.m,1)); # need to check why this is happening\n",
    "        self.X = LogisticRegression.addIntercept(self.X); # to be checked for documentation for how class name used in constructor\n",
    "        self.theta = self.initializeTheta();\n",
    "#         print(\"constructor\",self.y.shape);\n",
    "        \n",
    "    def initializeTheta(self):\n",
    "        return np.zeros((self.n + 1, 1));\n",
    "    \n",
    "    @staticmethod\n",
    "    def addIntercept(X):\n",
    "        m = X.shape[0];\n",
    "        X = np.hstack((np.ones((m,1)), X));\n",
    "        return X;\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-1 * z));\n",
    "\n",
    "    def buildModel(self):\n",
    "        \n",
    "        fminOutput = fmin_bfgs(\n",
    "                            self.costFunction,\n",
    "                            self.theta,\n",
    "                            self.gradFunction,\n",
    "                            disp=True,\n",
    "                            maxiter=400,\n",
    "                            full_output = True,\n",
    "                            retall=True\n",
    "                        );\n",
    "        opTheta = fminOutput[0];\n",
    "        opTheta = opTheta.reshape((self.n + 1, 1));\n",
    "        return opTheta, fminOutput\n",
    "        \n",
    "    \n",
    "        \n",
    "    def costFunction(self, theta):\n",
    "        \n",
    "        theta = np.reshape(theta,(self.n + 1, 1));\n",
    "        \n",
    "        z = np.dot(self.X, theta);\n",
    "        h = LogisticRegression.sigmoid(z);\n",
    "#         print(\"cost function before\",self.y.shape);\n",
    "        J = np.add(\n",
    "                np.multiply(\n",
    "                    (1/self.m) \n",
    "                    , np.sum( \n",
    "                            np.subtract(\n",
    "                                np.multiply(\n",
    "                                    np.multiply(-1, self.y),\n",
    "                                    np.log(h)\n",
    "                                )\n",
    "                                , np.multiply(\n",
    "                                    np.subtract(1, self.y), \n",
    "                                    np.log(np.subtract(1, h))\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                   )  \n",
    "                , np.multiply(\n",
    "                    (self.lam/self.m) \n",
    "                    , np.sum(\n",
    "                        np.square(\n",
    "                            np.vstack(\n",
    "                                [0, theta[1:]]\n",
    "                            )\n",
    "                        )\n",
    "                    ) \n",
    "                )\n",
    "        );\n",
    "#         print(\"cost function after\",self.y.shape);\n",
    "        return J;\n",
    "    \n",
    "    def gradFunction(self, theta):\n",
    "        \n",
    "#         print(\"grad function before\",self.y.shape);\n",
    "        theta = np.reshape(theta,(self.n + 1, 1));\n",
    "        \n",
    "#         print(\"grad function after reshaping\",self.y.shape);\n",
    "        z = np.dot(self.X, theta);\n",
    "        h = LogisticRegression.sigmoid(z);\n",
    "        \n",
    "        grad = ( \n",
    "                    np.multiply(\n",
    "                        (1/self.m) \n",
    "                        , (\n",
    "                           np.dot(\n",
    "                               np.transpose(self.X)\n",
    "                                ,np.subtract(h, self.y)\n",
    "                           )\n",
    "                        )\n",
    "                    )\n",
    "                ) + \\\n",
    "                ( \n",
    "                    np.multiply(\n",
    "                        (self.lam/self.m)\n",
    "                        , np.vstack(\n",
    "                            [0, theta[1:]]\n",
    "                        )\n",
    "                    )\n",
    "                );\n",
    "#         print(\"grad function after function formula\",self.y.shape);\n",
    "        grad = np.asarray(grad).reshape((self.n + 1,));\n",
    "#         print(\"grad function after function formula and reshaping\",self.y.shape);\n",
    "        return grad;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
